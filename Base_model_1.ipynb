{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rQgBdgOURzs1",
    "outputId": "e59bff03-a5c3-4bce-d0ad-8715878c62b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qiskit==0.44.1\n",
      "  Downloading qiskit-0.44.1-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting qiskit-terra==0.25.1 (from qiskit==0.44.1)\n",
      "  Downloading qiskit_terra-0.25.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting rustworkx>=0.13.0 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting numpy>=1.17 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ply>=3.10 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
      "Requirement already satisfied: psutil>=5 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra==0.25.1->qiskit==0.44.1) (7.0.0)\n",
      "Collecting scipy>=1.5 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy>=1.3 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra==0.25.1->qiskit==0.44.1) (2.9.0.post0)\n",
      "Collecting stevedore>=3.0.0 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting symengine<0.10,>=0.9 (from qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading symengine-0.9.2-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/sabdh/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit-terra==0.25.1->qiskit==0.44.1) (1.17.0)\n",
      "Collecting pbr>=2.0.0 (from stevedore>=3.0.0->qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.3->qiskit-terra==0.25.1->qiskit==0.44.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from pbr>=2.0.0->stevedore>=3.0.0->qiskit-terra==0.25.1->qiskit==0.44.1) (65.5.0)\n",
      "Downloading qiskit-0.44.1-py3-none-any.whl (8.2 kB)\n",
      "Downloading qiskit_terra-0.25.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading symengine-0.9.2-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pbr-6.1.1-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ply, mpmath, sympy, symengine, pbr, numpy, dill, stevedore, scipy, rustworkx, qiskit-terra, qiskit\n",
      "Successfully installed dill-0.4.0 mpmath-1.3.0 numpy-2.2.5 pbr-6.1.1 ply-3.11 qiskit-0.44.1 qiskit-terra-0.25.1 rustworkx-0.16.0 scipy-1.15.2 stevedore-5.4.1 symengine-0.9.2 sympy-1.14.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pylatexenc==3.0.0 (from versions: 1.0.macosx-10.6-x86_64, 0.9, 1.0, 1.0.post1, 1.1, 1.2, 1.3, 1.4, 1.5, 2.0, 2.0.post1, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 2.10, 3.0a13, 3.0a14, 3.0a15, 3.0a17, 3.0a18, 3.0a19, 3.0a21, 3.0a22, 3.0a23, 3.0a24, 3.0a25, 3.0a26, 3.0a27, 3.0a28, 3.0a29, 3.0a30, 3.0a31, 3.0a32)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pylatexenc==3.0.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting qiskit-machine-learning==0.6.1\n",
      "  Downloading qiskit_machine_learning-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: qiskit-terra>=0.22.2 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-machine-learning==0.6.1) (0.25.1)\n",
      "Requirement already satisfied: scipy>=1.4 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-machine-learning==0.6.1) (1.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-machine-learning==0.6.1) (2.2.5)\n",
      "Requirement already satisfied: psutil>=5 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-machine-learning==0.6.1) (7.0.0)\n",
      "Collecting scikit-learn>=0.20.0 (from qiskit-machine-learning==0.6.1)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting fastdtw (from qiskit-machine-learning==0.6.1)\n",
      "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.1.0 in /usr/local/lib/python3.11/site-packages (from qiskit-machine-learning==0.6.1) (65.5.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-machine-learning==0.6.1) (0.4.0)\n",
      "Requirement already satisfied: rustworkx>=0.13.0 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (0.16.0)\n",
      "Requirement already satisfied: ply>=3.10 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (3.11)\n",
      "Requirement already satisfied: sympy>=1.3 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (2.9.0.post0)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (5.4.1)\n",
      "Requirement already satisfied: symengine<0.10,>=0.9 in /home/sabdh/.local/lib/python3.11/site-packages (from qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (0.9.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=0.20.0->qiskit-machine-learning==0.6.1)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.20.0->qiskit-machine-learning==0.6.1)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/sabdh/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.0->qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (1.17.0)\n",
      "Requirement already satisfied: pbr>=2.0.0 in /home/sabdh/.local/lib/python3.11/site-packages (from stevedore>=3.0.0->qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (6.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sabdh/.local/lib/python3.11/site-packages (from sympy>=1.3->qiskit-terra>=0.22.2->qiskit-machine-learning==0.6.1) (1.3.0)\n",
      "Downloading qiskit_machine_learning-0.6.1-py3-none-any.whl (148 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: fastdtw\n",
      "  Building wheel for fastdtw (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastdtw: filename=fastdtw-0.3.4-py3-none-any.whl size=3610 sha256=fe96ea09383595704afb9ae114eb445e79a5d5e85c656d8e12db27fa7dce7168\n",
      "  Stored in directory: /home/sabdh/.cache/pip/wheels/5c/8a/f6/fd3df9a9714677410a5ccbf3ca519e66db4a54a1c46ea95332\n",
      "Successfully built fastdtw\n",
      "Installing collected packages: threadpoolctl, joblib, fastdtw, scikit-learn, qiskit-machine-learning\n",
      "Successfully installed fastdtw-0.3.4 joblib-1.4.2 qiskit-machine-learning-0.6.1 scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow==2.12.0\n",
      "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.12.0)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.12.0)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=2.0 (from tensorflow==2.12.0)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.12.0)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.12.0)\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting h5py>=2.9.0 (from tensorflow==2.12.0)\n",
      "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
      "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.12.0)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
      "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.12.0)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/sabdh/.local/lib/python3.11/site-packages (from tensorflow==2.12.0) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
      "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from tensorflow==2.12.0) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/sabdh/.local/lib/python3.11/site-packages (from tensorflow==2.12.0) (1.17.0)\n",
      "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
      "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.12.0)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/sabdh/.local/lib/python3.11/site-packages (from tensorflow==2.12.0) (4.13.2)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
      "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.12.0)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow==2.12.0)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.5.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.2,>=0.5.1 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.5.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
      "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
      "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
      "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
      "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: scipy>=1.9 in /home/sabdh/.local/lib/python3.11/site-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.2)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading google_auth-2.39.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/sabdh/.local/lib/python3.11/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sabdh/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sabdh/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sabdh/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sabdh/.local/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/sabdh/.local/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.39.0-py2.py3-none-any.whl (212 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.2/106.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, numpy, markdown, keras, grpcio, google-pasta, gast, cachetools, absl-py, rsa, requests-oauthlib, pyasn1-modules, ml_dtypes, h5py, astunparse, jaxlib, google-auth, jax, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "Successfully installed absl-py-2.2.2 astunparse-1.6.3 cachetools-5.5.2 flatbuffers-25.2.10 gast-0.4.0 google-auth-2.39.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 libclang-18.1.1 markdown-3.8 ml_dtypes-0.5.1 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.4.0 protobuf-4.25.7 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.12.3 tensorboard-data-server-0.7.2 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.0.1 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.14.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.23.5 in /home/sabdh/.local/lib/python3.11/site-packages (1.23.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting filelock (from torch==2.0.1)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/sabdh/.local/lib/python3.11/site-packages (from torch==2.0.1) (4.13.2)\n",
      "Requirement already satisfied: sympy in /home/sabdh/.local/lib/python3.11/site-packages (from torch==2.0.1) (1.14.0)\n",
      "Collecting networkx (from torch==2.0.1)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/sabdh/.local/lib/python3.11/site-packages (from torch==2.0.1) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1)\n",
      "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (65.5.0)\n",
      "Requirement already satisfied: wheel in /home/sabdh/.local/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1)\n",
      "  Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
      "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sabdh/.local/lib/python3.11/site-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/sabdh/.local/lib/python3.11/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, filelock, cmake, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-4.0.0 filelock-3.18.0 lit-18.1.8 networkx-3.4.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tqdm==4.66.1\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pip'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m package \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mqiskit\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpylatexenc\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mqiskit-machine-learning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mshow\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.returncode == \u001b[32m0\u001b[39m:\n\u001b[32m     19\u001b[39m         version = [line \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m result.stdout.split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m line.startswith(\u001b[33m'\u001b[39m\u001b[33mVersion:\u001b[39m\u001b[33m'\u001b[39m)][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:548\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    545\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstdout\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m    546\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mstderr\u001b[39m\u001b[33m'\u001b[39m] = PIPE\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    550\u001b[39m         stdout, stderr = process.communicate(\u001b[38;5;28minput\u001b[39m, timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:1026\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m   1022\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_mode:\n\u001b[32m   1023\u001b[39m             \u001b[38;5;28mself\u001b[39m.stderr = io.TextIOWrapper(\u001b[38;5;28mself\u001b[39m.stderr,\n\u001b[32m   1024\u001b[39m                     encoding=encoding, errors=errors)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   1036\u001b[39m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m.stdin, \u001b[38;5;28mself\u001b[39m.stdout, \u001b[38;5;28mself\u001b[39m.stderr)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/subprocess.py:1955\u001b[39m, in \u001b[36mPopen._execute_child\u001b[39m\u001b[34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[39m\n\u001b[32m   1953\u001b[39m     err_msg = os.strerror(errno_num)\n\u001b[32m   1954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1955\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[32m   1956\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1957\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'pip'"
     ]
    }
   ],
   "source": [
    "# Install Qiskit and related packages\n",
    "!pip3 install qiskit==0.44.1\n",
    "!pip3 install pylatexenc==3.0.0\n",
    "!pip3 install qiskit-machine-learning==0.6.1\n",
    "\n",
    "# Install TensorFlow and related packages\n",
    "!pip3 install tensorflow==2.12.0\n",
    "!pip3 install numpy==1.23.5\n",
    "\n",
    "# Install PyTorch and other utilities\n",
    "!pip3 install torch==2.0.1\n",
    "!pip3 install tqdm==4.66.1\n",
    "\n",
    "# Print installation status to confirm\n",
    "import subprocess\n",
    "for package in ['qiskit', 'pylatexenc', 'qiskit-machine-learning', 'tensorflow', 'numpy', 'torch', 'tqdm']:\n",
    "    result = subprocess.run(['pip', 'show', package], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        version = [line for line in result.stdout.split('\\n') if line.startswith('Version:')][0]\n",
    "        print(f\"{package}: {version}\")\n",
    "    else:\n",
    "        print(f\"{package}: Not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.6.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install qiskit==0.44.1\n",
    "# !pip install pylatexenc==3.0.0\n",
    "# !pip install qiskit-machine-learning==0.6.1\n",
    "\n",
    "# # Install TensorFlow and NumPy\n",
    "# !pip install tensorflow==2.12.0\n",
    "# !pip install numpy==1.23.5\n",
    "\n",
    "# # Install PyTorch and other utilities\n",
    "# !pip install torch==2.0.1\n",
    "# !pip install tqdm==4.66.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Test your imports\n",
    "from qiskit import *\n",
    "from qiskit import QuantumCircuit\n",
    "import qiskit.circuit.library as circuit_library\n",
    "import qiskit.quantum_info as qi\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit.circuit.library import EfficientSU2\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import torch\n",
    "from qiskit.circuit import ParameterVector\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from tqdm import tqdm\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czT7D0_kRzmv"
   },
   "outputs": [],
   "source": [
    "# Restart the runtime after installation\n",
    "import os\n",
    "os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fbx4ukklRziZ",
    "outputId": "ba7bbfee-c82d-479e-add1-bbdd29366ca9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 02:06:03.900073: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 02:06:03.958130: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-28 02:06:04.779203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic imports successful!\n",
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "from qiskit import QuantumCircuit\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "print(\"Basic imports successful!\")\n",
    "\n",
    "# Now try your full import list\n",
    "from qiskit import *\n",
    "from qiskit import QuantumCircuit\n",
    "import qiskit.circuit.library as circuit_library\n",
    "import qiskit.quantum_info as qi\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit.circuit.library import EfficientSU2\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import torch\n",
    "from qiskit.circuit import ParameterVector\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from tqdm import tqdm\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b6Lc7m-iRzeJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f6mYyQ7ORzYM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ra_z6VwWLIxV"
   },
   "outputs": [],
   "source": [
    "from qiskit import *\n",
    "# Qiskit module\n",
    "from qiskit import QuantumCircuit\n",
    "import qiskit.circuit.library as circuit_library\n",
    "import qiskit.quantum_info as qi\n",
    "#from qiskit import execute\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit.circuit.library import EfficientSU2\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "import torch\n",
    "from qiskit.circuit import ParameterVector\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sO8CMIIvLIum"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.2 in /home/sabdh/.local/lib/python3.11/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/sabdh/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/sabdh/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxDzmVWhCZqL",
    "outputId": "50bc44bb-36d1-4c30-b47c-54f39ed55afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RQGjPtjkCe1Z"
   },
   "outputs": [],
   "source": [
    "# path='/content/gdrive/MyDrive/NLP_projects/Lucas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "vGdQ6oU_CghZ",
    "outputId": "b476f906-425f-425d-c8a0-650550350314"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NODE</th>\n",
       "      <th>Label</th>\n",
       "      <th>rat</th>\n",
       "      <th>common</th>\n",
       "      <th>use</th>\n",
       "      <th>examin</th>\n",
       "      <th>pathogenesi</th>\n",
       "      <th>retinopathi</th>\n",
       "      <th>mous</th>\n",
       "      <th>studi</th>\n",
       "      <th>...</th>\n",
       "      <th>kidney</th>\n",
       "      <th>urinari</th>\n",
       "      <th>myocardi</th>\n",
       "      <th>meal</th>\n",
       "      <th>ica</th>\n",
       "      <th>locus</th>\n",
       "      <th>tcell</th>\n",
       "      <th>depress</th>\n",
       "      <th>bone</th>\n",
       "      <th>mutat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12187484</td>\n",
       "      <td>1</td>\n",
       "      <td>46.967448</td>\n",
       "      <td>14.349229</td>\n",
       "      <td>5.880063</td>\n",
       "      <td>9.687707</td>\n",
       "      <td>31.58066</td>\n",
       "      <td>85.445293</td>\n",
       "      <td>33.85124</td>\n",
       "      <td>8.777305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2344352</td>\n",
       "      <td>1</td>\n",
       "      <td>11.808958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.392080</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14654069</td>\n",
       "      <td>1</td>\n",
       "      <td>51.131572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.334490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16443886</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2684155</td>\n",
       "      <td>1</td>\n",
       "      <td>15.307909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NODE  Label        rat     common       use    examin  pathogenesi  \\\n",
       "0  12187484      1  46.967448  14.349229  5.880063  9.687707     31.58066   \n",
       "1   2344352      1  11.808958   0.000000  7.392080  0.000000      0.00000   \n",
       "2  14654069      1  51.131572   0.000000  5.334490  0.000000      0.00000   \n",
       "3  16443886      2   0.000000   0.000000  0.000000  0.000000      0.00000   \n",
       "4   2684155      1  15.307909   0.000000  0.000000  0.000000      0.00000   \n",
       "\n",
       "   retinopathi      mous     studi  ...  kidney  urinari  myocardi  meal  ica  \\\n",
       "0    85.445293  33.85124  8.777305  ...     0.0      0.0       0.0   0.0  0.0   \n",
       "1     0.000000   0.00000  0.000000  ...     0.0      0.0       0.0   0.0  0.0   \n",
       "2     0.000000   0.00000  0.000000  ...     0.0      0.0       0.0   0.0  0.0   \n",
       "3     0.000000   0.00000  0.000000  ...     0.0      0.0       0.0   0.0  0.0   \n",
       "4     0.000000   0.00000  0.000000  ...     0.0      0.0       0.0   0.0  0.0   \n",
       "\n",
       "   locus  tcell  depress  bone  mutat  \n",
       "0    0.0    0.0      0.0   0.0    0.0  \n",
       "1    0.0    0.0      0.0   0.0    0.0  \n",
       "2    0.0    0.0      0.0   0.0    0.0  \n",
       "3    0.0    0.0      0.0   0.0    0.0  \n",
       "4    0.0    0.0      0.0   0.0    0.0  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CSV file\n",
    "df_2= pd.read_csv('final_dataset_pubmed.csv')\n",
    "\n",
    "# Check the first few rows\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4XkHK6GeCgd6"
   },
   "outputs": [],
   "source": [
    "df=df_2[df_2['Label'].isin([1, 2])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f3V-PTGlCgbv"
   },
   "outputs": [],
   "source": [
    "df= df.fillna(0).round().astype('Int64', errors='ignore')  # Fill NaNs with 0, round, and convert to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "eI55XYqrCgZW",
    "outputId": "d770752e-1e3c-4766-d58f-930cabdf34a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NODE</th>\n",
       "      <th>Label</th>\n",
       "      <th>rat</th>\n",
       "      <th>common</th>\n",
       "      <th>use</th>\n",
       "      <th>examin</th>\n",
       "      <th>pathogenesi</th>\n",
       "      <th>retinopathi</th>\n",
       "      <th>mous</th>\n",
       "      <th>studi</th>\n",
       "      <th>...</th>\n",
       "      <th>kidney</th>\n",
       "      <th>urinari</th>\n",
       "      <th>myocardi</th>\n",
       "      <th>meal</th>\n",
       "      <th>ica</th>\n",
       "      <th>locus</th>\n",
       "      <th>tcell</th>\n",
       "      <th>depress</th>\n",
       "      <th>bone</th>\n",
       "      <th>mutat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12187484</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>85</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2344352</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14654069</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16443886</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2684155</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NODE  Label  rat  common  use  examin  pathogenesi  retinopathi  mous  \\\n",
       "0  12187484      1   47      14    6      10           32           85    34   \n",
       "1   2344352      1   12       0    7       0            0            0     0   \n",
       "2  14654069      1   51       0    5       0            0            0     0   \n",
       "3  16443886      2    0       0    0       0            0            0     0   \n",
       "4   2684155      1   15       0    0       0            0            0     0   \n",
       "\n",
       "   studi  ...  kidney  urinari  myocardi  meal  ica  locus  tcell  depress  \\\n",
       "0      9  ...       0        0         0     0    0      0      0        0   \n",
       "1      0  ...       0        0         0     0    0      0      0        0   \n",
       "2      0  ...       0        0         0     0    0      0      0        0   \n",
       "3      0  ...       0        0         0     0    0      0      0        0   \n",
       "4      0  ...       0        0         0     0    0      0      0        0   \n",
       "\n",
       "   bone  mutat  \n",
       "0     0      0  \n",
       "1     0      0  \n",
       "2     0      0  \n",
       "3     0      0  \n",
       "4     0      0  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d1FUeuhCqd9",
    "outputId": "8d1b9b6c-c10f-4fb3-fb34-178c91aba839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NODE       0\n",
      "Label      0\n",
      "rat        0\n",
      "common     0\n",
      "use        0\n",
      "          ..\n",
      "locus      0\n",
      "tcell      0\n",
      "depress    0\n",
      "bone       0\n",
      "mutat      0\n",
      "Length: 502, dtype: int64\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "print(df.isnull().values.any())  # Returns True if any NaN is present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsVySQhMCqWa",
    "outputId": "9ce2ca0e-5b95-489e-875e-15cefd2a2c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rat  common  use  examin  pathogenesi  retinopathi  mous  studi  anim  \\\n",
      "0   47      14    6      10           32           85    34      9    49   \n",
      "1   12       0    7       0            0            0     0      0    15   \n",
      "2   51       0    5       0            0            0     0      0    22   \n",
      "3    0       0    0       0            0            0     0      0     0   \n",
      "4   15       0    0       0            0            0     0      0    40   \n",
      "\n",
      "   model  ...  kidney  urinari  myocardi  meal  ica  locus  tcell  depress  \\\n",
      "0     31  ...       0        0         0     0    0      0      0        0   \n",
      "1      0  ...       0        0         0     0    0      0      0        0   \n",
      "2      0  ...       0        0         0     0    0      0      0        0   \n",
      "3     19  ...       0        0         0     0    0      0      0        0   \n",
      "4      0  ...       0        0         0     0    0      0      0        0   \n",
      "\n",
      "   bone  mutat  \n",
      "0     0      0  \n",
      "1     0      0  \n",
      "2     0      0  \n",
      "3     0      0  \n",
      "4     0      0  \n",
      "\n",
      "[5 rows x 500 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    2\n",
      "4    1\n",
      "Name: Label, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['Label','NODE'])  # Assuming 'Label' is the target\n",
    "y = df['Label']\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eeSrbQA4CtN4"
   },
   "outputs": [],
   "source": [
    "X_np = X.to_numpy()\n",
    "y_np = y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OAqYWS7CtKf",
    "outputId": "8259e0f3-809b-47b0-c0f6-edae041897bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (9582, 500), y_train shape: (9582,)\n",
      "X_test shape: (2396, 500), y_test shape: (2396,)\n"
     ]
    }
   ],
   "source": [
    "# # Handle missing values (e.g., fill with the mean of each column)\n",
    "# X = X.fillna(X.mean())\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the splits\n",
    "print(f'X_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
    "print(f'X_test shape: {x_test.shape}, y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKsyy2IcCy5s",
    "outputId": "c8cbdb5c-5590-4361-95ca-4a25b30f9b73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cs7B_nN-Cy2O",
    "outputId": "feae4702-e1d0-4851-951f-d72fd234d55a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5SeUfmtCyy3",
    "outputId": "12d3c895-7c36-4b05-90fa-de738a914fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xQ7ozMuQCtH5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "selected_num = 15000  # Total samples to be selected\n",
    "\n",
    "# Identify unique class labels dynamically (should return {1, 2, 3})\n",
    "unique_classes = np.unique(y_train)\n",
    "num_classes = len(unique_classes)  # Number of unique classes\n",
    "samples_per_class = selected_num // num_classes  # Equal split per class\n",
    "\n",
    "# Initialize storage for new dataset\n",
    "new_x_train, new_y_train = [], []\n",
    "class_counts = {label: 0 for label in unique_classes}  # Track counts dynamically\n",
    "\n",
    "indx = 0\n",
    "while len(new_x_train) < selected_num and indx < len(y_train):\n",
    "    label = y_train[indx]\n",
    "\n",
    "    if class_counts[label] < samples_per_class:  # Ensure balanced selection\n",
    "        new_x_train.append(x_train[indx])\n",
    "        new_y_train.append(y_train[indx])\n",
    "        class_counts[label] += 1  # Update count for the label\n",
    "\n",
    "    indx += 1  # Move to the next sample\n",
    "\n",
    "# Repeat the same logic for test data\n",
    "new_x_test, new_y_test = [], []\n",
    "class_counts_test = {label: 0 for label in unique_classes}\n",
    "\n",
    "indx = 0\n",
    "while len(new_x_test) < selected_num and indx < len(y_test):\n",
    "    label = y_test[indx]\n",
    "\n",
    "    if class_counts_test[label] < samples_per_class:\n",
    "        new_x_test.append(x_test[indx])\n",
    "        new_y_test.append(y_test[indx])\n",
    "        class_counts_test[label] += 1\n",
    "\n",
    "    indx += 1  # Move to the next sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFfB3t8rDAv_",
    "outputId": "75db4339-e773-4eed-df52-4d5b3b327683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500]\n"
     ]
    }
   ],
   "source": [
    "print([len(x) for x in new_x_train])  # Check the length of each element\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "n5O-yiTYDAsh"
   },
   "outputs": [],
   "source": [
    "x_train=np.array(new_x_train)#[:128]\n",
    "y_train=np.array(new_y_train)#[:128]\n",
    "\n",
    "x_test=np.array(new_x_test)#[:128]\n",
    "y_test=np.array(new_y_test)#[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UxzjXem9DAp7"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Get the word index from the IMDB dataset\n",
    "# word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# # Ensure that \"special\" words are mapped into human-readable terms\n",
    "# word_index = {k: (v + 3) for k, v in word_index.items()}\n",
    "# word_index[\"<PAD>\"] = 0\n",
    "# word_index[\"<START>\"] = 1\n",
    "# word_index[\"<UNKNOWN>\"] = 2\n",
    "# word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# # Perform reverse word lookup\n",
    "# reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "# # Function to decode a review\n",
    "# def decode_review(text):\n",
    "#     return ' '.join([reverse_word_index.get(i, \"<UNKNOWN>\") for i in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aja1Md5oDG-8",
    "outputId": "f8891865-efaf-4376-d42e-1c5ce9384701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum review length: 500\n",
      "Minimum review length: 500\n",
      "Mean review length: 500.00\n",
      "x_train size: 9582, x_test size: 2396\n",
      "\n",
      "Machine Readable Review\n",
      "  Review Text (Numeric): [0 0 11 9 0 81 0 11 0 0 0 0 0 7 0 0 0 0 11 0 7 0 0 0 0 0 0 12 0 0 0 0 0 0\n",
      " 0 0 0 4 0 0 0 0 0 0 6 0 11 0 0 16 0 0 0 0 0 0 9 0 0 0 0 8 0 0 0 0 0 7 0 0\n",
      " 0 13 13 0 0 0 0 0 0 0 0 0 0 0 0 0 35 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 82 8 0 13 0 0 0 0 0 0 0 0 10 0 0 0 0 0 0 0 0 0 19 8 0 12 10 0 0 0 0\n",
      " 3 0 0 14 15 0 0 0 8 37 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 10 12 0 0 0 0 6 0 0 0 0 0 14 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 20 0 0 10 0 0 0 0 0 0\n",
      " 0 0 0 23 0 0 0 22 0 0 0 0 0 0 23 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 39 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 14 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 13 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 57 0 31 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 34 34 0 0 0 0 0 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0]\n",
      "  Review Sentiment (Class Label): 2\n",
      "\n",
      "Human Readable Review\n",
      "  Review Sentiment: Diabetes Mellitus Type 1\n"
     ]
    }
   ],
   "source": [
    "# Define class names corresponding to labels {1, 2, 3}\n",
    "class_names = [\"Diabetes Mellitus, Experimental\", \"Diabetes Mellitus Type 1\", \"Diabetes Mellitus Type 2\"]\n",
    "\n",
    "# Concatenate test and training datasets\n",
    "allreviews = np.concatenate((x_train, x_test), axis=0)\n",
    "\n",
    "# Compute review lengths\n",
    "review_lengths = [len(x) for x in allreviews]\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"Maximum review length: {max(review_lengths)}\")\n",
    "print(f\"Minimum review length: {min(review_lengths)}\")\n",
    "print(f\"Mean review length: {np.mean(review_lengths):.2f}\")\n",
    "\n",
    "# Check dataset sizes before indexing\n",
    "print(f\"x_train size: {len(x_train)}, x_test size: {len(x_test)}\")\n",
    "\n",
    "# Select an index within valid range\n",
    "index_to_print = 10\n",
    "if index_to_print >= len(x_train):\n",
    "    print(f\"Error: x_train only has {len(x_train)} samples. Choose a lower index.\")\n",
    "else:\n",
    "    # Display machine-readable review\n",
    "    print(\"\\nMachine Readable Review\")\n",
    "    print(f\"  Review Text (Numeric): {x_train[index_to_print]}\")\n",
    "    print(f\"  Review Sentiment (Class Label): {y_train[index_to_print]}\")\n",
    "\n",
    "    # Display human-readable review\n",
    "    print(\"\\nHuman Readable Review\")\n",
    "   # print(f\"  Review Text: {decode_review(x_train[index_to_print])}\")\n",
    "\n",
    "    # Display corresponding class label\n",
    "    sentiment_index = y_train[index_to_print]\n",
    "    if sentiment_index in {1, 2, 3}:  # Check for valid labels\n",
    "        print(f\"  Review Sentiment: {class_names[sentiment_index - 1]}\")  # Adjust index (1-based to 0-based)\n",
    "    else:\n",
    "        print(f\"Error: Sentiment index {sentiment_index} is not valid (Expected: 1, 2, or 3).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWrKiEerDG7g",
    "outputId": "94932d82-1def-426e-d62b-5abdd2401d38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Training Review Data: (9582, 500)\n",
      "Shape Training Class Data: (9582,)\n",
      "Shape Test Review Data: (2396, 500)\n",
      "Shape Test Class Data: (2396,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Correct import\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "\n",
    "# The length of reviews\n",
    "review_length = 500\n",
    "\n",
    "# Padding / truncating our reviews\n",
    "x_train = pad_sequences(x_train, maxlen=review_length, padding='pre', truncating='pre')\n",
    "x_test = pad_sequences(x_test, maxlen=review_length, padding='pre', truncating='pre')\n",
    "\n",
    "# Check the size of our datasets. Review data for both test and training should\n",
    "# contain 25000 reviews of 200 integers (after padding). Class data should contain 25000 values,\n",
    "# one for each review. Class values are 0 or 1, indicating a negative or positive review.\n",
    "print(\"Shape Training Review Data: \" + str(x_train.shape))\n",
    "print(\"Shape Training Class Data: \" + str(y_train.shape))  # Fixed this line\n",
    "print(\"Shape Test Review Data: \" + str(x_test.shape))\n",
    "print(\"Shape Test Class Data: \" + str(y_test.shape))\n",
    "\n",
    "# Note padding is added to the start of the review (by default, padding='pre')\n",
    "print(\"\")\n",
    "# print(\"Human Readable Review Text (post padding): \" + decode_review(x_train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "mx6X4VyIDR26"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Transforming the training data (input features)\n",
    "new_X_train = []\n",
    "for seq in x_train:\n",
    "    # Transform each sequence (scale, shift, multiply by pi, reshape)\n",
    "    a = torch.tensor(np.pi * (seq / 1000 - 0.5))\n",
    "    new_X_train.append(torch.reshape(a, (25, 20)))\n",
    "\n",
    "# Stack the transformed sequences into a tensor\n",
    "new_X_train = torch.stack(new_X_train)\n",
    "\n",
    "# Transforming the training labels (target values)\n",
    "new_y_train = []\n",
    "for seq in y_train:\n",
    "    # Assuming labels are in the range 1, 2, 3, ..., convert them to zero-based indices\n",
    "    new_y_train.append(torch.tensor(seq - 1))  # Shift labels to 0, 1, 2, ...\n",
    "\n",
    "# Stack the labels into a tensor\n",
    "label = torch.stack(new_y_train)\n",
    "\n",
    "# Transforming the test data (input features)\n",
    "new_X_test = []\n",
    "for seq in x_test:\n",
    "    # Transform each sequence (scale, shift, multiply by pi, reshape)\n",
    "    a = torch.tensor(np.pi * (seq / 1000 - 0.5))\n",
    "    new_X_test.append(torch.reshape(a, (25, 20)))\n",
    "\n",
    "# Stack the transformed sequences into a tensor\n",
    "new_X_test = torch.stack(new_X_test)\n",
    "\n",
    "# Transforming the test labels (target values)\n",
    "new_y_test = []\n",
    "for seq in y_test:\n",
    "    # Assuming labels are in the range 1, 2, 3, ..., convert them to zero-based indices\n",
    "    new_y_test.append(torch.tensor(seq - 1))  # Shift labels to 0, 1, 2, ...\n",
    "\n",
    "# Stack the labels into a tensor\n",
    "label_test = torch.stack(new_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "SL69XLGwDRzZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import qiskit\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# If using a GPU, ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "from qiskit.utils import algorithm_globals\n",
    "algorithm_globals.random_seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2ueNTt1DRxW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYSdiYs7DG4l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB5zOpZMDEhy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tUjMM57WCGmC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class QSAL_qiskit(torch.nn.Module):\n",
    "    def __init__(self,S,n,Denc,D):\n",
    "        \"\"\"\n",
    "        # input: input data\n",
    "        # weight: trainable parameter\n",
    "        # n: # of of qubits\n",
    "        # d: embedding dimension which is equal to n(Denc+2)\n",
    "        # Denc: the # number of layers for encoding\n",
    "        # D: the # of layers of variational layers\n",
    "        # type \"K\": key, \"Q\": Query, \"V\": value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.seq_num=S\n",
    "        self.init_params_Q=torch.nn.Parameter(torch.stack([(np.pi/4) * (2 * torch.randn(n*(D+2)) - 1) for _ in range(self.seq_num)]))\n",
    "        self.init_params_K=torch.nn.Parameter(torch.stack([(np.pi/4) * (2 * torch.randn(n*(D+2)) - 1) for _ in range(self.seq_num)]))\n",
    "        self.init_params_V=torch.nn.Parameter(torch.stack([(np.pi/4) * (2 * torch.randn(n*(D+2)) - 1) for _ in range(self.seq_num)]))\n",
    "        self.params_input=[ParameterVector('IN'+str(i),n*(Denc+2)) for i in range(self.seq_num)]\n",
    "        self.params_Q=[ParameterVector('Q'+str(i),n*(D+2)) for i in range(self.seq_num)]\n",
    "        self.params_K=[ParameterVector('K'+str(i),n*(D+2)) for i in range(self.seq_num)]\n",
    "        self.params_V=[ParameterVector('V'+str(i),n*(D+2)) for i in range(self.seq_num)]\n",
    "        self.num_q=n\n",
    "        self.Denc=Denc\n",
    "        self.D=D\n",
    "        self.d=n*(Denc+2)\n",
    "        self.bit_string_Z=SparsePauliOp.from_list([('I'*(self.num_q-1)+'Z', 1)])\n",
    "        self.pauli_strings=[]\n",
    "        for i in range(self.d):\n",
    "            string=['I']*self.num_q\n",
    "            while string==['I']*self.num_q:\n",
    "                for j in range(self.num_q):\n",
    "                    a=random.randint(0, 4)\n",
    "                    if a==0:\n",
    "                        continue\n",
    "                    elif a==1:\n",
    "                        string[j]='X'\n",
    "                    elif a==2:\n",
    "                        string[j]='Y'\n",
    "                    elif a==3:\n",
    "                        string[j]='Z'\n",
    "            self.pauli_strings.append(SparsePauliOp.from_list([(\"\".join(string), 1)]))\n",
    "\n",
    "        Q_qnn=[EstimatorQNN(circuit=self.QSAL_cir(\"Q\",i),observables=[self.bit_string_Z], input_params=self.params_input[i], weight_params=self.params_Q[i]) for i in range(self.seq_num)]\n",
    "        K_qnn=[EstimatorQNN(circuit=self.QSAL_cir(\"K\",i),observables=[self.bit_string_Z], input_params=self.params_input[i], weight_params=self.params_K[i]) for i in range(self.seq_num)]\n",
    "        V_qnn=[EstimatorQNN(circuit=self.QSAL_cir(\"V\",i),observables=self.pauli_strings, input_params=self.params_input[i], weight_params=self.params_V[i]) for i in range(self.seq_num)]\n",
    "\n",
    "        self.Q_models=[TorchConnector(Q_qnn[i], initial_weights=self.init_params_Q[i]) for i in range(self.seq_num)]\n",
    "        self.K_models=[TorchConnector(K_qnn[i], initial_weights=self.init_params_K[i]) for i in range(self.seq_num)]\n",
    "        self.V_models=[TorchConnector(V_qnn[i], initial_weights=self.init_params_V[i]) for i in range(self.seq_num)]\n",
    "\n",
    "    def QSAL_cir(self,type,indx):\n",
    "\n",
    "        qc=QuantumCircuit(self.num_q)\n",
    "        if type==\"Q\":\n",
    "            self.Feature_map(qc,self.params_input[indx])\n",
    "            self.ansatz(qc,self.params_Q[indx])\n",
    "\n",
    "        elif type==\"K\":\n",
    "            self.Feature_map(qc,self.params_input[indx])\n",
    "            self.ansatz(qc,self.params_K[indx])\n",
    "\n",
    "        elif type==\"V\":\n",
    "            self.Feature_map(qc,self.params_input[indx])\n",
    "            self.ansatz(qc,self.params_V[indx])\n",
    "\n",
    "        return qc\n",
    "\n",
    "    def Feature_map(self,qc,params):\n",
    "        indx=0\n",
    "        for j in range(self.num_q):\n",
    "            qc.rx(params[indx],j)\n",
    "            qc.ry(params[indx+1],j)\n",
    "            indx+=2\n",
    "        for i in range(self.Denc):\n",
    "            for j in range(self.num_q):\n",
    "                qc.cx(j,(j+1)%self.num_q)\n",
    "\n",
    "            for j in range(self.num_q):\n",
    "                #qc.rx(params[indx],j)\n",
    "                qc.ry(params[indx],j)\n",
    "                indx+=1\n",
    "\n",
    "\n",
    "    def ansatz(self,qc,params):\n",
    "        indx=0\n",
    "        for j in range(self.num_q):\n",
    "            qc.rx(params[indx],j)\n",
    "            qc.ry(params[indx+1],j)\n",
    "            indx+=2\n",
    "        for i in range(self.D):\n",
    "            for j in range(self.num_q):\n",
    "                qc.cx(j,(j+1)%self.num_q)\n",
    "\n",
    "            for j in range(self.num_q):\n",
    "                #qc.rx(params[indx],j)\n",
    "                qc.ry(params[indx],j)\n",
    "                indx+=1\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        Q_output=torch.stack([self.Q_models[i](input[:,i]) for i in range(self.seq_num)])\n",
    "        K_output=torch.stack([self.K_models[i](input[:,i]) for i in range(self.seq_num)])\n",
    "        V_output=torch.stack([self.V_models[i](input[:,i]) for i in range(self.seq_num)])\n",
    "        batch_size=len(input)\n",
    "        Q_output=Q_output.transpose(0,2).repeat((self.seq_num,1,1))\n",
    "        K_output=K_output.transpose(0,2).repeat((self.seq_num,1,1)).transpose(0,2)\n",
    "        #print(V_output.size())\n",
    "        #Q_grid, K_grid=torch.meshgrid(Q_output, K_output, indexing='ij')\n",
    "        alpha=torch.exp(-(Q_output-K_output)**2)\n",
    "        alpha=alpha.transpose(0,1)\n",
    "        V_output=V_output.transpose(0,1)\n",
    "        output=[]\n",
    "\n",
    "        for i in range(self.seq_num):\n",
    "\n",
    "            Sum_a=torch.sum(alpha[:,i,:],-1)\n",
    "            div_sum_a=(1/Sum_a).repeat(self.d,self.seq_num,1).transpose(0,2)\n",
    "\n",
    "            Sum_w=torch.sum(alpha[:,:,i].repeat((self.d,1,1)).transpose(0,2).transpose(0,1)*V_output*div_sum_a,1)\n",
    "            output.append(Sum_w)\n",
    "        return input+torch.stack(output).transpose(0,1)\n",
    "\n",
    "\n",
    "class QSANN_qiskit(torch.nn.Module):\n",
    "    def __init__(self,S,n,Denc,D,num_layers):\n",
    "        \"\"\"\n",
    "        # input: input data\n",
    "        # weight: trainable parameter\n",
    "        # n: # of of qubits\n",
    "        # d: embedding dimension which is equal to n(Denc+2)\n",
    "        # Denc: the # number of layers for encoding\n",
    "        # D: the # of layers of variational layers\n",
    "        # type \"K\": key, \"Q\": Query, \"V\": value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.qsal_lst=[QSAL_qiskit(S,n,Denc,D) for _ in range(num_layers)]\n",
    "        self.qnn=nn.Sequential(*self.qsal_lst)\n",
    "\n",
    "    def forward(self,input):\n",
    "        print(f\"Input shape: {input.shape}\")\n",
    "\n",
    "        return self.qnn(input)\n",
    "\n",
    "class QSANN_text_classifier(torch.nn.Module):\n",
    "    def __init__(self,S,n,Denc,D,num_layers):\n",
    "        \"\"\"\n",
    "        # input: input data\n",
    "        # weight: trainable parameter\n",
    "        # n: # of of qubits\n",
    "        # d: embedding dimension which is equal to n(Denc+2)\n",
    "        # Denc: the # number of layers for encoding\n",
    "        # D: the # of layers of variational layers\n",
    "        # type \"K\": key, \"Q\": Query, \"V\": value\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.Qnn=QSANN_qiskit(S,n,Denc,D,num_layers)\n",
    "        self.final_layer=nn.Linear(n*(Denc+2)*S, 1)\n",
    "        self.final_layer=self.final_layer.float()\n",
    "\n",
    "    def forward(self,input):\n",
    "\n",
    "        x=self.Qnn(input)\n",
    "        x=torch.flatten(x,start_dim=1)\n",
    "\n",
    "        return torch.sigmoid(self.final_layer(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "EhXYmsaIDiy5"
   },
   "outputs": [],
   "source": [
    "# model=QSANN_text_classifier(5,2,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "I74R7DecDivZ"
   },
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(lr=0.03, params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "7a7WfH58DitB"
   },
   "outputs": [],
   "source": [
    "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "lmAfWq6aDmiK"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "EdcIsh0ADmej"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = (torch.round(torch.sign(preds-0.5))+1)//2\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSoXsf4dDsss",
    "outputId": "23b92711-556a-4f74-bd8e-29086261f588"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabdh/.local/lib/python3.11/site-packages/qiskit_machine_learning/connectors/torch_connector.py:312: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self._weights.data = torch.tensor(initial_weights, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "# In your model initialization, S should equal 25 (sequence length)\n",
    "# and n*(Denc+2) should equal 20 (feature dimension)\n",
    "\n",
    "# Example:\n",
    "S = 25  # Sequence number - should match your input sequence length\n",
    "n = 2   # Number of qubits\n",
    "Denc = 8  # Should be set so that n*(Denc+2) = 20\n",
    "D = 2   # Number of variational layers\n",
    "num_layers = 1  # Number of QSANN layers\n",
    "\n",
    "# This should satisfy: n*(Denc+2) = 20, so we get:\n",
    "# 2*(8+2) = 2*10 = 20\n",
    "\n",
    "model = QSANN_text_classifier(S=S, n=n, Denc=Denc, D=D, num_layers=num_layers)\n",
    "# model = QSANN_text_classifier(10,2,8,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "UX_CAv3AD0Z0"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=0.005, params=model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCxYUdXxD3XI",
    "outputId": "e0be9202-7bae-4caf-85be-871342cc9bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAAxzbnbD3Oa",
    "outputId": "f215d4e1-8e32-48ba-aed9-faeb65f89295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9582"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train.shape[0]  # Number of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "M--N6ZojXp-z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "MjR4tsFZD8jT",
    "outputId": "75a69cef-987c-4e5f-cf5e-b7901e37faca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 1/40\n",
      "Accuracy: 0.8125\n",
      "Loss: 90.1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                               | 1/40 [10:10<6:36:40, 610.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 2/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 76.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████                                                                             | 2/40 [20:30<6:30:02, 615.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 3/40\n",
      "Accuracy: 0.7188\n",
      "Loss: 79.7079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████                                                                           | 3/40 [28:35<5:43:05, 556.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 4/40\n",
      "Accuracy: 0.6562\n",
      "Loss: 72.7768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                         | 4/40 [36:40<5:17:00, 528.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 5/40\n",
      "Accuracy: 0.7188\n",
      "Loss: 79.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|██████████▏                                                                      | 5/40 [44:46<4:59:14, 513.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 6/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 76.2297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|████████████▏                                                                    | 6/40 [52:49<4:44:52, 502.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 7/40\n",
      "Accuracy: 0.5938\n",
      "Loss: 65.7913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▊                                                                 | 7/40 [1:00:54<4:33:19, 496.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 8/40\n",
      "Accuracy: 0.6250\n",
      "Loss: 69.2454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▊                                                               | 8/40 [1:08:57<4:22:36, 492.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 9/40\n",
      "Accuracy: 0.7500\n",
      "Loss: 83.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████████████▊                                                             | 9/40 [1:16:57<4:12:31, 488.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 10/40\n",
      "Accuracy: 0.8125\n",
      "Loss: 90.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|███████████████████▌                                                          | 10/40 [1:25:00<4:03:28, 486.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 11/40\n",
      "Accuracy: 0.6250\n",
      "Loss: 69.2306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████▍                                                        | 11/40 [1:33:04<3:54:53, 485.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 12/40\n",
      "Accuracy: 0.5312\n",
      "Loss: 58.8046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▍                                                      | 12/40 [1:41:07<3:46:18, 484.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 13/40\n",
      "Accuracy: 0.6250\n",
      "Loss: 69.1793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▎                                                    | 13/40 [1:49:11<3:38:10, 484.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 14/40\n",
      "Accuracy: 0.7188\n",
      "Loss: 79.6028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████▎                                                  | 14/40 [1:57:17<3:30:10, 485.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 15/40\n",
      "Accuracy: 0.6250\n",
      "Loss: 69.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|█████████████████████████████▎                                                | 15/40 [2:05:23<3:22:18, 485.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 16/40\n",
      "Accuracy: 0.8438\n",
      "Loss: 93.4781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▏                                              | 16/40 [2:13:27<3:13:57, 484.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 17/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 76.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████████████████████▏                                            | 17/40 [2:21:31<3:05:43, 484.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 18/40\n",
      "Accuracy: 0.6562\n",
      "Loss: 72.6608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|███████████████████████████████████                                           | 18/40 [2:29:35<2:57:37, 484.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 19/40\n",
      "Accuracy: 0.7188\n",
      "Loss: 79.5903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████                                         | 19/40 [2:37:41<2:49:41, 484.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 20/40\n",
      "Accuracy: 0.6562\n",
      "Loss: 72.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████                                       | 20/40 [2:45:46<2:41:42, 485.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 21/40\n",
      "Accuracy: 0.5312\n",
      "Loss: 58.7629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████████▉                                     | 21/40 [2:53:50<2:33:31, 484.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 22/40\n",
      "Accuracy: 0.5625\n",
      "Loss: 62.1666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|██████████████████████████████████████████▉                                   | 22/40 [3:01:56<2:25:30, 485.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 23/40\n",
      "Accuracy: 0.8125\n",
      "Loss: 89.9589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|████████████████████████████████████████████▊                                 | 23/40 [3:09:59<2:17:16, 484.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 24/40\n",
      "Accuracy: 0.7500\n",
      "Loss: 82.9662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▊                               | 24/40 [3:18:06<2:09:20, 485.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 25/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 76.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████████████████████████████▊                             | 25/40 [3:26:07<2:01:00, 484.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 26/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 76.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████████████████████████████████████████████████▋                           | 26/40 [3:34:12<1:53:01, 484.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 27/40\n",
      "Accuracy: 0.6250\n",
      "Loss: 69.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████████▋                         | 27/40 [3:42:17<1:44:56, 484.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 28/40\n",
      "Accuracy: 0.5312\n",
      "Loss: 58.7248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████▌                       | 28/40 [3:50:22<1:36:56, 484.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 29/40\n",
      "Accuracy: 0.6562\n",
      "Loss: 72.5138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████▌                     | 29/40 [3:58:24<1:28:43, 483.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 30/40\n",
      "Accuracy: 0.5938\n",
      "Loss: 65.5440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████▌                   | 30/40 [4:06:30<1:20:44, 484.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 31/40\n",
      "Accuracy: 0.6250\n",
      "Loss: 69.0457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████████████▍                 | 31/40 [4:14:33<1:12:37, 484.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 32/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 76.0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████▍               | 32/40 [4:22:39<1:04:35, 484.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 33/40\n",
      "Accuracy: 0.7188\n",
      "Loss: 79.4654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|██████████████████████████████████████████████████████████████████              | 33/40 [4:30:42<56:29, 484.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 34/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 75.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████            | 34/40 [4:38:47<48:26, 484.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 35/40\n",
      "Accuracy: 0.5938\n",
      "Loss: 65.5971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████████████████████████████████████████          | 35/40 [4:46:46<40:14, 482.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 36/40\n",
      "Accuracy: 0.6875\n",
      "Loss: 75.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████        | 36/40 [4:54:47<32:09, 482.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 37/40\n",
      "Accuracy: 0.7188\n",
      "Loss: 79.4908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████      | 37/40 [5:02:49<24:05, 481.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 38/40\n",
      "Accuracy: 0.6250\n",
      "Loss: 69.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████████████████████████████████████████████    | 38/40 [5:10:51<16:04, 482.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 39/40\n",
      "Accuracy: 0.5938\n",
      "Loss: 65.5180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████████████████████████████████████████████  | 39/40 [5:18:54<08:02, 482.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 25, 20])\n",
      "\n",
      "Epoch 40/40\n",
      "Accuracy: 0.7812\n",
      "Loss: 86.4065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 40/40 [5:26:53<00:00, 490.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of classes (2 for binary classification)\n",
    "num_classes = 1  # Single output for binary classification\n",
    "\n",
    "# Prepare the training data\n",
    "new_X_train = []\n",
    "for seq in x_train:\n",
    "    a = torch.tensor(np.pi * (seq / 1000 - 0.5))  # Transform input data\n",
    "    new_X_train.append(torch.reshape(a, (25, 20)))  # Reshape each sequence to (25, 20)\n",
    "\n",
    "new_y_train = []\n",
    "for seq in y_train:\n",
    "    new_y_train.append(torch.tensor(seq))  # Convert labels to tensors\n",
    "\n",
    "# Stack lists of tensors into single tensors\n",
    "new_X_train = torch.stack(new_X_train)  # Shape: (num_samples, 25, 20)\n",
    "new_y_train = torch.stack(new_y_train)  # Shape: (num_samples,)\n",
    "\n",
    "# Ensure new_X_test and new_y_test are also converted\n",
    "new_X_test = []\n",
    "for seq in x_test:\n",
    "    a = torch.tensor(np.pi * (seq / 1000 - 0.5))\n",
    "    new_X_test.append(torch.reshape(a, (25, 20)))\n",
    "\n",
    "new_y_test = []\n",
    "for seq in y_test:\n",
    "    new_y_test.append(torch.tensor(seq))\n",
    "\n",
    "new_X_test = torch.stack(new_X_test)  # Shape: (num_samples, 25, 20)\n",
    "new_y_test = torch.stack(new_y_test)  # Shape: (num_samples,)\n",
    "\n",
    "# Convert target labels to zero-indexed (1 -> 0, 2 -> 1, 3 -> 2) for binary classification\n",
    "new_y_train = (new_y_train != 1).long()  # Convert 1 -> 0, and other labels to 1 for binary classification\n",
    "new_y_test = (new_y_test != 1).long()  # Convert 1 -> 0, and other labels to 1 for binary classification\n",
    "\n",
    "# Define the model (fix the output layer for binary classification)\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.fc = nn.Linear(25 * 20, num_classes)  # Single logit output for binary classification\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.shape[0], -1)  # Flatten input from (25, 20) to (500,)\n",
    "#         return self.fc(x)  # Output shape: (batch_size, 1)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "# model = Model()\n",
    "# criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss with logits\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop settings\n",
    "num_epochs = 40\n",
    "batch_size = 32\n",
    "num_samples = new_X_train.shape[0]  # Number of training samples\n",
    "\n",
    "# Training loop\n",
    "for iepoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Generate random indices for the batch\n",
    "    idx = torch.randperm(num_samples)[:batch_size]  # Select random batch\n",
    "\n",
    "    # Extract the batch\n",
    "    X_batch = new_X_train[idx].float()  # Ensure input is float\n",
    "    y_batch = new_y_train[idx]  # Labels should be binary (0 or 1)\n",
    "\n",
    "    # Model output: logits (single raw score)\n",
    "    logits = model(X_batch)  # Shape: (batch_size, 1)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(logits.squeeze(), y_batch.float())  # BCEWithLogitsLoss expects logits, not probabilities\n",
    "\n",
    "    # Predicted class (apply a threshold of 0.5 to get binary prediction)\n",
    "    predicted_classes = (torch.sigmoid(logits).squeeze() > 0.5).long()  # Threshold at 0.5\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = (predicted_classes == y_batch).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "\n",
    "    # Log results\n",
    "    print(f\"\\nEpoch {iepoch + 1}/{num_epochs}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Backpropagation and optimizer step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path ='nlp_wsl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name=path+'QSANN_qiskit_model_1'\n",
    "torch.save(model.state_dict(),file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all training data\n",
    "X_batch = new_X_train.float()  # Ensure the input is float\n",
    "y_batch = new_y_train  # These are your labels\n",
    "\n",
    "# Get model predictions\n",
    "logits = model(X_batch)  # Shape would be [batch_size, 1]\n",
    "\n",
    "# Compute loss - note we squeeze to match dimensions for BCEWithLogitsLoss\n",
    "loss = criterion(logits.squeeze(), y_batch.float())\n",
    "\n",
    "# Calculate accuracy manually\n",
    "predicted_probs = torch.sigmoid(logits.squeeze())  # Convert logits to probabilities\n",
    "predicted_classes = (predicted_probs > 0.5).long()  # Apply threshold\n",
    "correct = (predicted_classes == y_batch).float()  # Compare with true labels\n",
    "acc = correct.mean().item()  # Average of correct predictions\n",
    "\n",
    "print('')\n",
    "print('Accuracy:', acc)\n",
    "print('')\n",
    "print('Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/sabdh/.local/lib/python3.11/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sabdh/.local/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/sabdh/.local/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/sabdh/.local/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.57.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.2.1 pyparsing-3.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test data...\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([128, 25, 20])\n",
      "Input shape: torch.Size([92, 25, 20])\n",
      "Accuracy: 0.6490\n",
      "Loss: 396.7468\n",
      "Precision: 0.6490\n",
      "Recall/Sensitivity: 1.0000\n",
      "Specificity: 0.0000\n",
      "F1 Score: 0.7871\n",
      "ROC AUC: 0.9375\n",
      "PR AUC: 0.9589\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 0, FP: 841\n",
      "FN: 0, TP: 1555\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, X_test, y_test, criterion, batch_size=128):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Process test data in batches to avoid memory issues\n",
    "    num_test_samples = X_test.shape[0]\n",
    "    num_batches = (num_test_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, num_test_samples)\n",
    "            \n",
    "            # Extract batch\n",
    "            X_batch = X_test[start_idx:end_idx].float()\n",
    "            y_batch = y_test[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(X_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(logits.squeeze(), y_batch.float())\n",
    "            total_loss += loss.item() * (end_idx - start_idx)\n",
    "            \n",
    "            # Predictions\n",
    "            predicted_probs = torch.sigmoid(logits.squeeze())\n",
    "            predicted_classes = (predicted_probs > 0.5).long()\n",
    "            \n",
    "            # Accumulate results\n",
    "            correct = (predicted_classes == y_batch).sum().item()\n",
    "            total_correct += correct\n",
    "            \n",
    "            # Store predictions and true labels for additional metrics\n",
    "            all_predictions.extend(predicted_probs.cpu().numpy())\n",
    "            all_true_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = total_correct / num_test_samples\n",
    "    avg_loss = total_loss / num_test_samples\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'loss': avg_loss,\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'true_labels': np.array(all_true_labels)\n",
    "    }\n",
    "\n",
    "# Additional evaluation metrics\n",
    "def calculate_metrics(results):\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve, auc, confusion_matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    y_true = results['true_labels']\n",
    "    y_pred_probs = results['predictions']\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # PR curve\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred_probs)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "    print(f\"Loss: {results['loss']:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall/Sensitivity: {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PR AUC: {pr_auc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"TN: {tn}, FP: {fp}\")\n",
    "    print(f\"FN: {fn}, TP: {tp}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Plot PR curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'specificity': specificity,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating model on test data...\")\n",
    "test_results = evaluate_model(model, new_X_test, new_y_test, criterion)\n",
    "metrics = calculate_metrics(test_results)\n",
    "\n",
    "# Save predictions and probabilities for further analysis\n",
    "test_predictions = (test_results['predictions'] > 0.5).astype(int)\n",
    "test_probabilities = test_results['predictions']\n",
    "\n",
    "# You can also save these predictions for later analysis\n",
    "np.save('test_predictions.npy', test_predictions)\n",
    "np.save('test_probabilities.npy', test_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name=path+'QSANN_qiskit_model_1'\n",
    "torch.save(model.state_dict(),file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_batch = new_X_train\n",
    "# predictions=model(X_batch.float()).squeeze(1)\n",
    "\n",
    "# loss = criterion(predictions, label.float())\n",
    "# acc = binary_accuracy(predictions,label)\n",
    "# print('')\n",
    "# print('Accuracy:',acc)\n",
    "# print('')\n",
    "# print(loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
